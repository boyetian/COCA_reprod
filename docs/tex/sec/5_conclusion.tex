\vspace{6pt}
\section{Conclusions}
% \vspace{-2pt}
In this paper, we aim to investigate the co-learning mechanism to enhance test-time adaptation (TTA). Notably, we uncover two key insights regarding different models: 1) They provide complementary knowledge from training, and 2) They exhibit varying robustness against noisy learning signals during testing. To this end, we propose COCA, a co-learning TTA approach that promotes bidirectional cooperation across models. COCA integrates complementary knowledge to mitigate biases and enables tailored adaptation strategies for each modelâ€™s strengths. Notably, a learnable temperature, $\tau$, is introduced to align the outputs from models of different sizes. Extensive experiments validate COCA's effectiveness across diverse models and offer new insights into TTA. For example, we show that a smaller model like ResNet-18, attributed to its robustness to noise at testing, can significantly enhance the TTA of a much larger model like ViT-large. 
% Moreover, collaboration between models with different architectures yields the most substantial performance gains. 
Additionally, COCA can also serve as a plug-and-play module that seamlessly complements traditional TTA methods for more effective and robust solutions. 

% Finally, we also verify COCA's applicability to multiple models, which leads to a consistent improvement in overall accuracy .

% 
% aligns the outputs of models with varying sizes and explores the complementary knowledge of a smaller model to enhance a larger model, ensuring a more effective and robust adaptation process.

% Extensive experiments validate COCA's effectiveness across diverse models and offer new insights into test-time adaptation (TTA). For example, a smaller model like ResNet-18, known for its robustness to noisy guidance at test time, can significantly guide a larger model such as ViT-large. Moreover, collaboration between models with different architectures yields the most substantial performance gains. Additionally, COCA can also serve as a plug-and-play module that can seamlessly complement traditional TTA methods, further enhancing their effectiveness. COCA's applicability to multiple models is also confirmed.




% In this paper, we introduce COCA, a novel test-time adaptation (TTA) approach that leverages cross-model collaboration to significantly enhance model performance. Unlike traditional one-way methods, such as the teacher-student framework, COCA fosters bi-directional cooperation between models, enabling mutual performance improvements. COCA not only integrates complementary knowledge from different models to mitigate individual biases, but also enables diverse adaptation strategies tailored to each model's unique strengths. Moreover, To enhance knowledge aggregation and co-adaptation, we introduce a learnable parameter $\tau$, which aligns the outputs of models with different sizes, ensuring a more effective and robust adaptation process. 

% Our experiments demonstrate that collaboration between models with different architectures yields the greatest performance gains. Smaller models not only boost the performance of larger models but also achieve notable improvements in their own accuracy. These findings highlight the potential of model collaboration and output integration in driving substantial performance improvements. COCA is a versatile framework that can seamlessly complement traditional TTA methods, boosting their effectiveness. Furthermore, the applicability of COCA across multiple models is also confirmed, as detailed in Section D of appendix. Through extensive experiments and analysis, we provide valuable insights into TTA and cross-model co-learning, offering a fresh perspective on overcoming the limitations of conventional adaptation techniques.


% In this paper, we introduce COCA, a novel test-time adaptation (TTA) approach that leverages cross-model collaboration to significantly enhance model performance. Unlike traditional one-way methods, such as the teacher-student framework, COCA promotes bi-directional cooperation between models, enabling mutual performance improvements. Our experiments demonstrate that collaboration between models with different architectures yields the greatest performance gains. Smaller models not only enhance the performance of larger models but also show notable improvements in their own accuracy. Additionally, we introduce a unique adaptive temperature scaling parameter to combine model outputs, resulting in further performance enhancements. These findings highlight the potential of model collaboration and output integration to achieve substantial performance gains. COCA is also a versatile framework that can seamlessly integrate with traditional TTA methods, boosting their effectiveness. Through extensive experiments and analysis, we offer valuable insights into TTA and cross-model co-learning, presenting a fresh perspective on overcoming the limitations of conventional adaptation techniques.