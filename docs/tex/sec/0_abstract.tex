
\begin{abstract}
Test-time Adaptation (TTA) adapts a given model to testing domain data with potential domain shifts through online unsupervised learning, yielding impressive performance. However, to date, existing TTA methods primarily focus on single-model adaptation. In this work, we investigate an intriguing question: how does cross-model knowledge influence the TTA process? Our findings reveal that, in TTA's unsupervised online setting, each model can provide complementary, confident knowledge to the others, even when there are substantial differences in model size. For instance, a smaller model like MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base (86.6M parameters). In light of this, we propose COCA, a \textbf{c}ross-m\textbf{o}del \textbf{c}o-le\textbf{a}rning framework for TTA, which mainly consists of two main strategies. 1) Co-adaptation adaptively integrates complementary knowledge from other models throughout the TTA process, reducing individual model biases. 2) Self-adaptation enhances each model’s unique strengths via unsupervised learning, enabling diverse adaptation to the target domain. Extensive experiments show that COCA, which can also serve as a plug-and-play module, significantly boosts existing SOTAs, on models with various sizes—including ResNets, ViTs, and Mobile-ViTs—via cross-model co-learned TTA. For example, with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy on ImageNet-C from 51.7\% to 64.5\%. The code will be publicly available.







% Test-time Adaptation (TTA) adapts a given model to test data with potential domain shifts through online unsupervised learning, leading to impressive performance. However, existing TTA methods primarily focus on single-model adaptation, where the source domain is represented by a single model. In this work, we explore an intriguing question: how does cross-model knowledge influence the TTA process? Our findings reveal that each model can provide complementary knowledge to others, even when there are significant differences in model size. For example, a smaller model like MobileViT (10.6M parameters) can effectively guide a larger model such as ViT-Base (86.6M parameters). In light of this, we propose COCA, a \textbf{c}ross-m\textbf{o}del \textbf{c}o-le\textbf{a}rning framework for TTA. COCA consists of two main strategies, i.e., co-adaptation and self-adaptation. Co-adaptation integrates the collaborative knowledge from another model to reduce individual model biases, while self-adaptation enhances each model’s unique strengths by self-supervised learning, enabling diverse adaptation to the target domain. Extensive experiments show that COCA, as a plug-and-play module, significantly improves the performance of existing state-of-the-art methods on various models—including ResNets, ViTs, and Mobile-ViTs—through cross-model co-learned TTA. For instance, with assistance from Mobile-ViT, COCA boosts the average adaptation accuracy of ViT-Base on ImageNet-C from 51.7\% to 64.5\%. The code will be made publicly available.



% Unlike existing methods that focus on single-model adaptation, in this work, we found that different pre-trained models can provide complementary knowledge to each other. Even when one model has very low original performance, such as the significant difference between ViT-Base and ResNet-18, their collaboration can still significantly improve adaptation. This suggests that utilizing different models, rather than just one, may significantly enhance the adaptation process. To this end, we introduce COCA, a \textbf{\underline{c}}ross-m\textbf{\underline{o}}del \textbf{\underline{c}}o-le\textbf{\underline{a}}rning approach designed to improve TTA by investigating the mutual promotion across different models. In COCA, each model leverages both its knowledge and the collaborative knowledge from the other model to enhance its development, while also contributing its knowledge to enrich the shared collaborative knowledge. Our extensive experiments demonstrate that COCA effectively harnesses both the individual strengths of each model and the benefits of cross-model collaboration, leading to significant improvements in TTA performance. The source code will be publicly available.

% We found that, unlike traditional co-learning in supervised settings—where similar model scales are typically required—in the unsupervised online setting, each model can provide complementary, confident knowledge to the others, even when there is a significant size difference. For example, a smaller model like MobileViT can still guide a larger model like ViT-Base.

% In COCA, each model benefits not only from its own knowledge but also from the collaborative knowledge shared with other models, thereby enhancing its performance while contributing to the collective knowledge pool.


% Test-time Adaptation (TTA) aims to adapt knowledge from a source model to a different but related domain in an online manner. Unlike existing methods that focus on single-model adaptation, in this work, we found that different pre-trained models can provide complementary knowledge to each other. Even when one model has very low original performance, such as the significant difference between ViT-Base and ResNet-18, their collaboration can still significantly improve adaptation. This suggests that utilizing different models, rather than just one, may significantly enhance the adaptation process. In this paper, we introduce COCA, a \textbf{\underline{c}}ross-m\textbf{\underline{o}}del \textbf{\underline{c}}o-le\textbf{\underline{a}}rning approach designed to improve TTA by investigating the mutual promotion across different models. In COCA, each model leverages both its knowledge and the collaborative knowledge from the other model to enhance its development, while also contributing its knowledge to enrich the shared collaborative knowledge. Our extensive experiments demonstrate that COCA effectively harnesses both the individual strengths of each model and the benefits of cross-model collaboration, leading to significant improvements in TTA performance. The source code will be publicly available. 

% Test-Time Adaptation (TTA) aims to adapt knowledge from a source model to a new, but related, target domain in an online manner. Most existing TTA methods rely on a single model for self-supervised learning in the target domain, with techniques like entropy minimization. However, these methods often overlook the limited amount of information a single model can leverage for effective adaptation. To address this limitation, we propose a novel strategy called COCA (\textbf{\underline{c}}ross-m\textbf{\underline{o}}del \textbf{\underline{c}}o-le\textbf{\underline{a}}rning). Specifically, for each target sample, different models exhibit distinct predictive behaviors. By decomposing the models' outputs into discrepancy and consensus, we facilitate collaborative learning, allowing models to mutually enhance each other's performance. In addition, we design an adaptive integration mechanism that effectively combines the outputs of models, leading to enhanced performance. Our extensive experiments demonstrate that COCA successfully harnesses the individual strengths of each model, as well as the benefits of cross-model collaboration, resulting in significant performance improvements for TTA. The source code will be made publicly available.

\end{abstract}
