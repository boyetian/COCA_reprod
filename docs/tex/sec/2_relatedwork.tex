\vspace{-0.11in}
\section{Related Works}
\vspace{-0.03in}
% The related literature of this work can be categorized into two main areas: test-time adaptation (TTA) and co-learning.

\paragraph{Test-Time Adaptation} The goal of Test-Time Adaptation (TTA) is to leverage pre-trained models and adapt them to OOD data through fine-tuning, achieving robust performance in the target domain~\cite{liang2024comprehensive}. Recently, various methods have been proposed to tackle TTA. TEA~\cite{yuan2024tea} transforms the source model into an energy-based classifier to mitigate domain shifts. AdaContrast~\cite{chen2022contrastive} combines contrastive learning and pseudo-labeling to address TTA. DomainAdapter~\cite{zhang2023domain} adaptively merges training and test statistics within normalization layers. AdaNPC~\cite{zhang2023adanpc} is a parameter-free TTA approach based on a K-Nearest Neighbor (KNN) classifier, using a voting mechanism to assign labels based on $k$ nearest samples from memory. In contrast to traditional approaches, CTTA-VDP ~\cite{gan2023decorate} introduces a homeostasis-based prompt adaptation strategy, freezing the source model parameters during continual TTA. Additionally, FOA~\cite{niu2024test} devise a fitness function that measures test-training statistic discrepancy and model prediction entropy. 
% TTAB \cite{zhao2023pitfalls} unveils three common pitfalls in prior TTA approaches under classification tasks, based on a large-scale open-sourced benchmark and thorough quantitative analysis.

Among the various TTA methods, entropy minimization has emerged as a prominent approach. For example, Tent~\cite{wang2020tent} minimizes the entropy of the model’s predictions on OOD data, adapting only the normalization layer parameters. Inspired by Tent, methods such as EATA~\cite{niu2022efficient}, MEMO~\cite{zhang2022memo}, SAR~\cite{niu2023towards}, DeYO~\cite{lee2024entropy}, and ROID~\cite{marsden2024universal2} have demonstrated strong performance in entropy-based TTA. However, most prior works leverage only the limited knowledge from a pre-trained model and thus result in constrained TTA efficacy, as shown in Table~\ref{mainres}.

% However, most prior works focus on training a single model for each source domain. 


% In contrast, our proposed method, COCA, emphasizes enabling two networks—originating from the same source domain but with different configurations—to collaborate during adaptation. This collaborative approach yields superior performance, as the combined models achieve better results than any individual model alone.
\vspace{-15pt}
\paragraph{Co-Learning} Traditional co-learning paradigms include the teacher-student framework~\cite{hu2022teacher, beyer2022knowledge}, multi-modal settings~\cite{yin2023crossmatch}, and ensemble learning~\cite{yang2023survey}. In the teacher-student setting, knowledge is transferred unidirectionally from a stronger teacher to a weaker student. In contrast, COCA operates bidirectionally, enhancing overall performance by mutually improving both models. A key distinction between COCA and multi-modal settings is that in COCA, all models share the same input and task. In typical ensemble learning, models make independent predictions that are later aggregated. However, COCA enables direct inter-model influence throughout the entire adaptation process, fostering deeper collaboration. 

In the field of domain adaptation which is related to TTA, CMT~\cite{cao2023contrastive} proposes contrastive mean teacher to maximize beneficial learning signals. Harmonious Teacher~\cite{deng2023harmonious} is a sample reweighing strategy based on the consistency of classification. TTA methods like CoTTA~\cite{wang2022continual}, RoTTA~\cite{yuan2023robust}, and RMT~\cite{dobler2023robust} have successfully applied the teacher-student paradigm within the same model, yielding strong performance. In unsupervised domain adaptation, a closely related approach is AML~\cite{zhou2023adaptive}, which tackles this issue by adaptively switching the roles of the teacher and student, thereby alleviating the challenge of selecting a powerful teacher model. However, in TTA context, determining when to switch these roles becomes more complex, primarily due to the inconsistency in confidence levels between the outputs of different models. This inconsistency makes it difficult to establish a clear criterion for role-switching.


% However, a key challenge in implementing a teacher-student architecture with different models in TTA is the difficulty of pre-defining a reliable and effective teacher model, as is typically done in traditional teacher-student networks.

% Our proposed COCA differs from existing co-learning methods by emphasizing collaborative improvement between models throughout the entire adaptation process. Unlike the traditional teacher-student framework~\cite{hu2022teacher, beyer2022knowledge}, where knowledge is transferred solely from a stronger teacher to a weaker student, COCA operates bidirectionally, enhancing overall performance by mutually improving both models. The key distinction between COCA and multi-modal~\cite{yin2023crossmatch} settings is that, in COCA, all models share the same input and task. In contrast to ensemble learning~\cite{yang2023survey}, where models typically make independent predictions that are later aggregated, COCA enables direct inter-model influence throughout the entire adaptation process, promoting deeper collaboration and dynamic knowledge exchange. By extending the performance boundaries of each model, COCA aims to achieve more effective real-time adaptation. The robustness of COCA is further ensured by the collaborative integration of the distinct advantages offered by different models.

% Our proposed COCA differs from existing TTA methods by emphasizing collaborative improvement between models. Unlike the traditional teacher-student framework~\cite{hu2022teacher, beyer2022knowledge}, where knowledge is transferred solely from a stronger teacher to a weaker student, COCA operates in a bidirectional manner, aiming to enhance overall performance while mutually improving both models. The robustness of COCA is ensured by the collaborative integration of the distinct advantages offered by different models. The key distinction between COCA and multi-modal~\cite{yin2023crossmatch} settings is that, in COCA, all models share the same input and task. In contrast to ensemble learning~\cite{yang2023survey}, where models typically make independent predictions that are later aggregated, COCA enables direct inter-model influence throughout the adaptation process, promoting deeper collaboration and dynamic knowledge exchange. By extending the performance boundaries of each model, we aim to achieve more effective real-time adaptation.

% \red{Overall performance?}