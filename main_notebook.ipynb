{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.12/site-packages (1.0.19)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (6.0.1)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (10.4.0)\n",
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.26.2)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/lib/python3.12/site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.12\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision timm numpy pyyaml pillow kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiny-imagenet-c.zip: 100%|██████████████████| 1.25G/1.25G [03:04<00:00, 7.30MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|███████████████████████| 750000/750000 [01:21<00:00, 9213.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset successfully downloaded and extracted to data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_with_progress(url, save_path):\n",
    "    # Send GET request\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()  # Check for HTTP errors\n",
    "    \n",
    "    # Get total file size in bytes\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    # Download with progress bar\n",
    "    with open(save_path, 'wb') as f, tqdm(\n",
    "        desc=os.path.basename(save_path),\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = f.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "# Create data directory if needed\n",
    "data_dir = 'data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download parameters\n",
    "url = 'https://www.kaggle.com/api/v1/datasets/download/luckyhathaway/tiny-imagenet-c'\n",
    "zip_path = os.path.join(data_dir, 'tiny-imagenet-c.zip')\n",
    "extract_path = os.path.join(data_dir)\n",
    "\n",
    "print(\"Downloading dataset...\")\n",
    "download_with_progress(url, zip_path)\n",
    "\n",
    "# Extract with progress\n",
    "print(\"\\nExtracting dataset...\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    file_list = zip_ref.namelist()\n",
    "    for file in tqdm(file_list, desc=\"Extracting\"):\n",
    "        zip_ref.extract(file, extract_path)\n",
    "\n",
    "# Clean up\n",
    "os.remove(zip_path)\n",
    "\n",
    "print(f\"\\nDataset successfully downloaded and extracted to {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.coca import COCA\n",
    "from data.imagenet_c import ImageNetC\n",
    "from scripts.test_accuracy import test_accuracy\n",
    "from models.resnet import resnet50\n",
    "from models.vit import vit_base_patch16_224\n",
    "from utils.augmentations import get_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_root': './data/Tiny-ImageNet-C', 'batch_size': 32, 'workers': 4, 'corruption': 'gaussian_noise', 'severity': 5, 'lr_anchor': 0.001, 'lr_aux': 0.00025, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "args = {\n",
    "    'data_root': './data/Tiny-ImageNet-C',\n",
    "    'batch_size': 32,\n",
    "    'workers': 4,\n",
    "    'corruption': 'gaussian_noise',\n",
    "    'severity': 5,\n",
    "    'lr_anchor': 0.001,\n",
    "    'lr_aux': 0.00025,\n",
    "    'momentum': 0.9\n",
    "}\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added blocks.0.norm1.weight to optimizer\n",
      "Added blocks.0.norm1.bias to optimizer\n",
      "Added blocks.0.norm2.weight to optimizer\n",
      "Added blocks.0.norm2.bias to optimizer\n",
      "Added blocks.1.norm1.weight to optimizer\n",
      "Added blocks.1.norm1.bias to optimizer\n",
      "Added blocks.1.norm2.weight to optimizer\n",
      "Added blocks.1.norm2.bias to optimizer\n",
      "Added blocks.2.norm1.weight to optimizer\n",
      "Added blocks.2.norm1.bias to optimizer\n",
      "Added blocks.2.norm2.weight to optimizer\n",
      "Added blocks.2.norm2.bias to optimizer\n",
      "Added blocks.3.norm1.weight to optimizer\n",
      "Added blocks.3.norm1.bias to optimizer\n",
      "Added blocks.3.norm2.weight to optimizer\n",
      "Added blocks.3.norm2.bias to optimizer\n",
      "Added blocks.4.norm1.weight to optimizer\n",
      "Added blocks.4.norm1.bias to optimizer\n",
      "Added blocks.4.norm2.weight to optimizer\n",
      "Added blocks.4.norm2.bias to optimizer\n",
      "Added blocks.5.norm1.weight to optimizer\n",
      "Added blocks.5.norm1.bias to optimizer\n",
      "Added blocks.5.norm2.weight to optimizer\n",
      "Added blocks.5.norm2.bias to optimizer\n",
      "Added blocks.6.norm1.weight to optimizer\n",
      "Added blocks.6.norm1.bias to optimizer\n",
      "Added blocks.6.norm2.weight to optimizer\n",
      "Added blocks.6.norm2.bias to optimizer\n",
      "Added blocks.7.norm1.weight to optimizer\n",
      "Added blocks.7.norm1.bias to optimizer\n",
      "Added blocks.7.norm2.weight to optimizer\n",
      "Added blocks.7.norm2.bias to optimizer\n",
      "Added blocks.8.norm1.weight to optimizer\n",
      "Added blocks.8.norm1.bias to optimizer\n",
      "Added blocks.8.norm2.weight to optimizer\n",
      "Added blocks.8.norm2.bias to optimizer\n",
      "Added blocks.9.norm1.weight to optimizer\n",
      "Added blocks.9.norm1.bias to optimizer\n",
      "Added blocks.9.norm2.weight to optimizer\n",
      "Added blocks.9.norm2.bias to optimizer\n",
      "Added blocks.10.norm1.weight to optimizer\n",
      "Added blocks.10.norm1.bias to optimizer\n",
      "Added blocks.10.norm2.weight to optimizer\n",
      "Added blocks.10.norm2.bias to optimizer\n",
      "Added blocks.11.norm1.weight to optimizer\n",
      "Added blocks.11.norm1.bias to optimizer\n",
      "Added blocks.11.norm2.weight to optimizer\n",
      "Added blocks.11.norm2.bias to optimizer\n",
      "Added norm.weight to optimizer\n",
      "Added norm.bias to optimizer\n",
      "Added bn1.weight to optimizer\n",
      "Added bn1.bias to optimizer\n",
      "Added layer1.0.bn1.weight to optimizer\n",
      "Added layer1.0.bn1.bias to optimizer\n",
      "Added layer1.0.bn2.weight to optimizer\n",
      "Added layer1.0.bn2.bias to optimizer\n",
      "Added layer1.0.bn3.weight to optimizer\n",
      "Added layer1.0.bn3.bias to optimizer\n",
      "Added layer1.0.downsample.1.weight to optimizer\n",
      "Added layer1.0.downsample.1.bias to optimizer\n",
      "Added layer1.1.bn1.weight to optimizer\n",
      "Added layer1.1.bn1.bias to optimizer\n",
      "Added layer1.1.bn2.weight to optimizer\n",
      "Added layer1.1.bn2.bias to optimizer\n",
      "Added layer1.1.bn3.weight to optimizer\n",
      "Added layer1.1.bn3.bias to optimizer\n",
      "Added layer1.2.bn1.weight to optimizer\n",
      "Added layer1.2.bn1.bias to optimizer\n",
      "Added layer1.2.bn2.weight to optimizer\n",
      "Added layer1.2.bn2.bias to optimizer\n",
      "Added layer1.2.bn3.weight to optimizer\n",
      "Added layer1.2.bn3.bias to optimizer\n",
      "Added layer2.0.bn1.weight to optimizer\n",
      "Added layer2.0.bn1.bias to optimizer\n",
      "Added layer2.0.bn2.weight to optimizer\n",
      "Added layer2.0.bn2.bias to optimizer\n",
      "Added layer2.0.bn3.weight to optimizer\n",
      "Added layer2.0.bn3.bias to optimizer\n",
      "Added layer2.0.downsample.1.weight to optimizer\n",
      "Added layer2.0.downsample.1.bias to optimizer\n",
      "Added layer2.1.bn1.weight to optimizer\n",
      "Added layer2.1.bn1.bias to optimizer\n",
      "Added layer2.1.bn2.weight to optimizer\n",
      "Added layer2.1.bn2.bias to optimizer\n",
      "Added layer2.1.bn3.weight to optimizer\n",
      "Added layer2.1.bn3.bias to optimizer\n",
      "Added layer2.2.bn1.weight to optimizer\n",
      "Added layer2.2.bn1.bias to optimizer\n",
      "Added layer2.2.bn2.weight to optimizer\n",
      "Added layer2.2.bn2.bias to optimizer\n",
      "Added layer2.2.bn3.weight to optimizer\n",
      "Added layer2.2.bn3.bias to optimizer\n",
      "Added layer2.3.bn1.weight to optimizer\n",
      "Added layer2.3.bn1.bias to optimizer\n",
      "Added layer2.3.bn2.weight to optimizer\n",
      "Added layer2.3.bn2.bias to optimizer\n",
      "Added layer2.3.bn3.weight to optimizer\n",
      "Added layer2.3.bn3.bias to optimizer\n",
      "Added layer3.0.bn1.weight to optimizer\n",
      "Added layer3.0.bn1.bias to optimizer\n",
      "Added layer3.0.bn2.weight to optimizer\n",
      "Added layer3.0.bn2.bias to optimizer\n",
      "Added layer3.0.bn3.weight to optimizer\n",
      "Added layer3.0.bn3.bias to optimizer\n",
      "Added layer3.0.downsample.1.weight to optimizer\n",
      "Added layer3.0.downsample.1.bias to optimizer\n",
      "Added layer3.1.bn1.weight to optimizer\n",
      "Added layer3.1.bn1.bias to optimizer\n",
      "Added layer3.1.bn2.weight to optimizer\n",
      "Added layer3.1.bn2.bias to optimizer\n",
      "Added layer3.1.bn3.weight to optimizer\n",
      "Added layer3.1.bn3.bias to optimizer\n",
      "Added layer3.2.bn1.weight to optimizer\n",
      "Added layer3.2.bn1.bias to optimizer\n",
      "Added layer3.2.bn2.weight to optimizer\n",
      "Added layer3.2.bn2.bias to optimizer\n",
      "Added layer3.2.bn3.weight to optimizer\n",
      "Added layer3.2.bn3.bias to optimizer\n",
      "Added layer3.3.bn1.weight to optimizer\n",
      "Added layer3.3.bn1.bias to optimizer\n",
      "Added layer3.3.bn2.weight to optimizer\n",
      "Added layer3.3.bn2.bias to optimizer\n",
      "Added layer3.3.bn3.weight to optimizer\n",
      "Added layer3.3.bn3.bias to optimizer\n",
      "Added layer3.4.bn1.weight to optimizer\n",
      "Added layer3.4.bn1.bias to optimizer\n",
      "Added layer3.4.bn2.weight to optimizer\n",
      "Added layer3.4.bn2.bias to optimizer\n",
      "Added layer3.4.bn3.weight to optimizer\n",
      "Added layer3.4.bn3.bias to optimizer\n",
      "Added layer3.5.bn1.weight to optimizer\n",
      "Added layer3.5.bn1.bias to optimizer\n",
      "Added layer3.5.bn2.weight to optimizer\n",
      "Added layer3.5.bn2.bias to optimizer\n",
      "Added layer3.5.bn3.weight to optimizer\n",
      "Added layer3.5.bn3.bias to optimizer\n",
      "Added layer4.0.bn1.weight to optimizer\n",
      "Added layer4.0.bn1.bias to optimizer\n",
      "Added layer4.0.bn2.weight to optimizer\n",
      "Added layer4.0.bn2.bias to optimizer\n",
      "Added layer4.0.bn3.weight to optimizer\n",
      "Added layer4.0.bn3.bias to optimizer\n",
      "Added layer4.0.downsample.1.weight to optimizer\n",
      "Added layer4.0.downsample.1.bias to optimizer\n",
      "Added layer4.1.bn1.weight to optimizer\n",
      "Added layer4.1.bn1.bias to optimizer\n",
      "Added layer4.1.bn2.weight to optimizer\n",
      "Added layer4.1.bn2.bias to optimizer\n",
      "Added layer4.1.bn3.weight to optimizer\n",
      "Added layer4.1.bn3.bias to optimizer\n",
      "Added layer4.2.bn1.weight to optimizer\n",
      "Added layer4.2.bn1.bias to optimizer\n",
      "Added layer4.2.bn2.weight to optimizer\n",
      "Added layer4.2.bn2.bias to optimizer\n",
      "Added layer4.2.bn3.weight to optimizer\n",
      "Added layer4.2.bn3.bias to optimizer\n",
      "Adapted on batch 10/313\n",
      "Adapted on batch 20/313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x13adcbce0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1582, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "anchor_model = vit_base_patch16_224(pretrained=True).to(device)\n",
    "aux_model = resnet50(pretrained=True).to(device)\n",
    "coca = COCA(anchor_model, aux_model, lr_anchor=args['lr_anchor'], lr_aux=args['lr_aux'], momentum=args['momentum'])\n",
    "\n",
    "transform_anchor = get_transform('vit_base_patch16_224')\n",
    "transform_aux = get_transform('resnet50')\n",
    "dataset = ImageNetC(\n",
    "    root=args['data_root'],\n",
    "    corruption_type=args['corruption'],\n",
    "    severity=args['severity'],\n",
    "    transform_anchor=transform_anchor,\n",
    "    transform_aux=transform_aux\n",
    ")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], num_workers=args['workers'], shuffle=True)\n",
    "\n",
    "for i, (images_anchor, images_aux, _) in enumerate(data_loader):\n",
    "    images_anchor = images_anchor.to(device)\n",
    "    images_aux = images_aux.to(device)\n",
    "    coca.update(images_anchor, images_aux)\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f'Adapted on batch {i+1}/{len(data_loader)}')\n",
    "\n",
    "accuracy = test_accuracy(coca, args['data_root'], args['batch_size'], args['workers'], args['corruption'], args['severity'])\n",
    "print(f'Accuracy on {args['corruption']} (severity {args['severity']}): {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
